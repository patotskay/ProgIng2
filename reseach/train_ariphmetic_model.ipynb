{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data üê±‚Äçüèç\n",
    "\n",
    "–î–∞–Ω–Ω—ã–µ –æ —Ä–µ—à–µ–Ω–∏–∏ –∞—Ä—Ñ–º–∏—Ç–∏—á–µ—Å–∫–∏—Ö –∑–∞–¥–∞—á"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import json\n",
    "\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> –ó–∞–≥—Ä—É–∂–∞–µ–º –¥–∞–Ω–Ω—ã–µ, —á–∏—Å—Ç–∏–º –¥–æ —Ç–µ—Ö, —É –∫–æ–≥–æ –∫–æ–Ω—Ç–µ–∫—Å—Ç $\\ge 5$\n",
    "\n",
    "> –î–µ–ª–∞–µ–º –∏—Ö –ø—Ä–∏–≥–æ–¥–Ω—ã–º–∏ –¥–ª—è DialoGPT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "N_CONTEXT = 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "–î–∞–Ω–Ω—ã—Ö –±—ã–ª–æ: 104131, –≤–∞–ª–∏–¥–Ω—ã—Ö (CONTEXT > 3): 59807\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>response</th>\n",
       "      <th>context/2</th>\n",
       "      <th>context/1</th>\n",
       "      <th>context/0</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>56516</th>\n",
       "      <td>1692 –º–∏–Ω—É—Å 825 —Ä–∞–≤–Ω—è–µ—Ç—Å—è 867</td>\n",
       "      <td>–ß—Ç–æ –ø–æ–ª—É—á–∏—Ç—Å—è, –µ—Å–ª–∏ –∏–∑ —ç—Ç–æ–≥–æ —á–∏—Å–ª–∞ –≤—ã—á–µ—Å—Ç—å 825?</td>\n",
       "      <td>(20+16)√ó47 —Ä–∞–≤–Ω–æ 1692</td>\n",
       "      <td>–ß—Ç–æ –ø–æ–ª—É—á–∏—Ç—Å—è, –µ—Å–ª–∏ –∫ 20 –ø—Ä–∏–±–∞–≤–∏—Ç—å 16, –∞ –ø–æ—Ç–æ–º...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>31894</th>\n",
       "      <td>–£–º–Ω–æ–∂–∞–µ–º 0 –Ω–∞ 55 –∏ –ø–æ–ª—É—á–∞–µ–º 0</td>\n",
       "      <td>–ü–æ–º–Ω–æ–∂—å –Ω–∞ 55 –∏ –Ω–∞–ø–∏—à–∏ —Ä–µ–∑—É–ª—å—Ç–∞—Ç.</td>\n",
       "      <td>–≠—Ç–æ –ø—Ä–æ–∏–∑–≤–µ–¥–µ–Ω–∏–µ —Ä–∞–≤–Ω–æ 0</td>\n",
       "      <td>–ù–∞–π–¥–∏ –ø—Ä–æ–∏–∑–≤–µ–¥–µ–Ω–∏–µ —á–∏—Å–µ–ª 9, 0 –∏ 8</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6478</th>\n",
       "      <td>9669</td>\n",
       "      <td>–ö–∞–∫–æ–µ —á–∏—Å–ª–æ –ø–æ–ª—É—á–∏—Ç—Å—è, –µ—Å–ª–∏ —Å–ø—Ä–∞–≤–∞ –¥–æ–ø–∏—Å–∞—Ç—å –∫ ...</td>\n",
       "      <td>–µ—Å–ª–∏ –∫ 81 –ø—Ä–∏–±–∞–≤–∏—Ç—å 15, –ø–æ–ª—É—á–∏—Ç—Å—è 96</td>\n",
       "      <td>–ß–µ–º—É —Ä–∞–≤–Ω–æ –∑–Ω–∞—á–µ–Ω–∏–µ –≤—ã—Ä–∞–∂–µ–Ω–∏—è m+n, –∫–æ–≥–¥–∞ m=81,...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20180</th>\n",
       "      <td>32</td>\n",
       "      <td>–ö–∞–∫–æ–µ —á–∏—Å–ª–æ –ø–æ–ª—É—á–∏—Ç—Å—è, –µ—Å–ª–∏ –≤ —ç—Ç–æ–º —á–∏—Å–ª–µ –∑–∞–º–µ–Ω...</td>\n",
       "      <td>72</td>\n",
       "      <td>–ß—Ç–æ –Ω–∞–¥–æ –ø–æ–¥—Å—Ç–∞–≤–∏—Ç—å –≤–º–µ—Å—Ç–æ u –≤ —É—Ä–∞–≤–Ω–µ–Ω–∏–∏ u-4=68?</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25163</th>\n",
       "      <td>–ü–æ–º–Ω–æ–∂–∏—Ç—å –µ–≥–æ –Ω–∞ 18</td>\n",
       "      <td>–ö–∞–∫ –ø—Ä–µ–≤—Ä–∞—Ç–∏—Ç—å —ç—Ç–æ —á–∏—Å–ª–æ –≤ 882 —Å –ø–æ–º–æ—â—å—é –∞—Ä–∏—Ñ–º...</td>\n",
       "      <td>84-35=49</td>\n",
       "      <td>–ß–µ–º—É —Ä–∞–≤–Ω—è–µ—Ç—Å—è X-Y, –µ—Å–ª–∏ X=84, –∞ Y=35?</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                            response  \\\n",
       "56516   1692 –º–∏–Ω—É—Å 825 —Ä–∞–≤–Ω—è–µ—Ç—Å—è 867   \n",
       "31894  –£–º–Ω–æ–∂–∞–µ–º 0 –Ω–∞ 55 –∏ –ø–æ–ª—É—á–∞–µ–º 0   \n",
       "6478                            9669   \n",
       "20180                             32   \n",
       "25163            –ü–æ–º–Ω–æ–∂–∏—Ç—å –µ–≥–æ –Ω–∞ 18   \n",
       "\n",
       "                                               context/2  \\\n",
       "56516    –ß—Ç–æ –ø–æ–ª—É—á–∏—Ç—Å—è, –µ—Å–ª–∏ –∏–∑ —ç—Ç–æ–≥–æ —á–∏—Å–ª–∞ –≤—ã—á–µ—Å—Ç—å 825?   \n",
       "31894                  –ü–æ–º–Ω–æ–∂—å –Ω–∞ 55 –∏ –Ω–∞–ø–∏—à–∏ —Ä–µ–∑—É–ª—å—Ç–∞—Ç.   \n",
       "6478   –ö–∞–∫–æ–µ —á–∏—Å–ª–æ –ø–æ–ª—É—á–∏—Ç—Å—è, –µ—Å–ª–∏ —Å–ø—Ä–∞–≤–∞ –¥–æ–ø–∏—Å–∞—Ç—å –∫ ...   \n",
       "20180  –ö–∞–∫–æ–µ —á–∏—Å–ª–æ –ø–æ–ª—É—á–∏—Ç—Å—è, –µ—Å–ª–∏ –≤ —ç—Ç–æ–º —á–∏—Å–ª–µ –∑–∞–º–µ–Ω...   \n",
       "25163  –ö–∞–∫ –ø—Ä–µ–≤—Ä–∞—Ç–∏—Ç—å —ç—Ç–æ —á–∏—Å–ª–æ –≤ 882 —Å –ø–æ–º–æ—â—å—é –∞—Ä–∏—Ñ–º...   \n",
       "\n",
       "                                  context/1  \\\n",
       "56516                 (20+16)√ó47 —Ä–∞–≤–Ω–æ 1692   \n",
       "31894              –≠—Ç–æ –ø—Ä–æ–∏–∑–≤–µ–¥–µ–Ω–∏–µ —Ä–∞–≤–Ω–æ 0   \n",
       "6478   –µ—Å–ª–∏ –∫ 81 –ø—Ä–∏–±–∞–≤–∏—Ç—å 15, –ø–æ–ª—É—á–∏—Ç—Å—è 96   \n",
       "20180                                    72   \n",
       "25163                              84-35=49   \n",
       "\n",
       "                                               context/0  \n",
       "56516  –ß—Ç–æ –ø–æ–ª—É—á–∏—Ç—Å—è, –µ—Å–ª–∏ –∫ 20 –ø—Ä–∏–±–∞–≤–∏—Ç—å 16, –∞ –ø–æ—Ç–æ–º...  \n",
       "31894                  –ù–∞–π–¥–∏ –ø—Ä–æ–∏–∑–≤–µ–¥–µ–Ω–∏–µ —á–∏—Å–µ–ª 9, 0 –∏ 8  \n",
       "6478   –ß–µ–º—É —Ä–∞–≤–Ω–æ –∑–Ω–∞—á–µ–Ω–∏–µ –≤—ã—Ä–∞–∂–µ–Ω–∏—è m+n, –∫–æ–≥–¥–∞ m=81,...  \n",
       "20180   –ß—Ç–æ –Ω–∞–¥–æ –ø–æ–¥—Å—Ç–∞–≤–∏—Ç—å –≤–º–µ—Å—Ç–æ u –≤ —É—Ä–∞–≤–Ω–µ–Ω–∏–∏ u-4=68?  \n",
       "25163             –ß–µ–º—É —Ä–∞–≤–Ω—è–µ—Ç—Å—è X-Y, –µ—Å–ª–∏ X=84, –∞ Y=35?  "
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data = json.load(open('../data/qa_arith.json', 'r', encoding='utf8'))\n",
    "idxs = []\n",
    "\n",
    "for idx, row in enumerate(data):\n",
    "    if len(row['conversation']) > N_CONTEXT:\n",
    "        idxs.append(idx)\n",
    "print(f\"–î–∞–Ω–Ω—ã—Ö –±—ã–ª–æ: {len(data)}, –≤–∞–ª–∏–¥–Ω—ã—Ö (CONTEXT > {N_CONTEXT}): {len(idxs)}\")\n",
    "\n",
    "contexted = []\n",
    "for sample in idxs:\n",
    "    row = []\n",
    "    for context in range(N_CONTEXT + 1):\n",
    "        row.append(data[sample]['conversation'][context])\n",
    "    contexted.append(row)\n",
    "\n",
    "columns = ['context/' + str(i) for i in range(N_CONTEXT)] + ['response']\n",
    "\n",
    "df = pd.DataFrame.from_records(contexted, columns=columns)\n",
    "df = df[reversed(df.columns)]\n",
    "df.sample(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>response</th>\n",
       "      <th>context/2</th>\n",
       "      <th>context/1</th>\n",
       "      <th>context/0</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>18854</th>\n",
       "      <td>–ò—Ö —Ç–∞–º –¥–≤–µ</td>\n",
       "      <td>–°–∫–æ–ª—å–∫–æ –≤ —ç—Ç–æ–º —á–∏—Å–ª–µ —Ü–∏—Ñ—Ä?</td>\n",
       "      <td>50</td>\n",
       "      <td>–ü—Ä–µ–∂–¥–µ –≤—Å–µ–≥–æ, 10*5 —á–µ–º—É —Ä–∞–≤–Ω—è–µ—Ç—Å—è?</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23118</th>\n",
       "      <td>114*95=10830</td>\n",
       "      <td>–ß—Ç–æ –ø–æ–ª—É—á–∏—Ç—Å—è, –µ—Å–ª–∏ —ç—Ç–æ —á–∏—Å–ª–æ —É–º–Ω–æ–∂–∏—Ç—å –Ω–∞ 95?</td>\n",
       "      <td>114</td>\n",
       "      <td>–ß—Ç–æ –Ω–∞–¥–æ –ø–æ–¥—Å—Ç–∞–≤–∏—Ç—å –≤–º–µ—Å—Ç–æ e, —á—Ç–æ–±—ã –≤—ã—Ä–∞–∂–µ–Ω–∏–µ ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14611</th>\n",
       "      <td>–†–µ–∑—É–ª—å—Ç–∞—Ç —Ä–∞–≤–µ–Ω 906</td>\n",
       "      <td>–ü—Ä–∏–±–∞–≤—å –∫ –Ω–µ–º—É 906, –∫–∞–∫–æ–π —Ä–µ–∑—É–ª—å—Ç–∞—Ç?</td>\n",
       "      <td>0</td>\n",
       "      <td>–í–ª–∞–¥–∏—Å–ª–∞–≤, —Å–∫–∞–∂–∏, –∞ —á–µ–º—É —Ä–∞–≤–Ω–æ 4-4?</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22726</th>\n",
       "      <td>1168 —É–º–Ω–æ–∂–∏—Ç—å –Ω–∞ 92 —Ä–∞–≤–Ω–æ 107456</td>\n",
       "      <td>–ß–µ–º—É —Ä–∞–≤–Ω–æ –ø—Ä–æ–∏–∑–≤–µ–¥–µ–Ω–∏–µ —ç—Ç–æ–≥–æ —á–∏—Å–ª–∞ –Ω–∞ 92?</td>\n",
       "      <td>–†–µ–∑—É–ª—å—Ç–∞—Ç —ç—Ç–æ–≥–æ –≤—ã—Ä–∞–∂–µ–Ω–∏—è —Ä–∞–≤–µ–Ω 1168</td>\n",
       "      <td>–ü–æ—Å—á–∏—Ç–∞–π 3087024 / ( 25938402 / ( 5807 - 5637 ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>44101</th>\n",
       "      <td>508, 607, 142, 321, 116, 58</td>\n",
       "      <td>–ó–∞—á–µ—Ä–∫–Ω–∏ –≤ —ç—Ç–æ–º —Å–ø–∏—Å–∫–µ —É –∫–∞–∂–¥–æ–≥–æ —á–∏—Å–ª–∞ –ø–µ—Ä–≤—É—é ...</td>\n",
       "      <td>9508, 5607, 3142, 2321, 2116, 458</td>\n",
       "      <td>–û—Ç—Å–æ—Ä—Ç–∏—Ä—É–π –ø–æ —É–±—ã–≤–∞–Ω–∏—é —á–∏—Å–ª–∞ –≤ —Å–ø–∏—Å–∫–µ 3142, 56...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                               response  \\\n",
       "18854                        –ò—Ö —Ç–∞–º –¥–≤–µ   \n",
       "23118                      114*95=10830   \n",
       "14611               –†–µ–∑—É–ª—å—Ç–∞—Ç —Ä–∞–≤–µ–Ω 906   \n",
       "22726  1168 —É–º–Ω–æ–∂–∏—Ç—å –Ω–∞ 92 —Ä–∞–≤–Ω–æ 107456   \n",
       "44101       508, 607, 142, 321, 116, 58   \n",
       "\n",
       "                                               context/2  \\\n",
       "18854                         –°–∫–æ–ª—å–∫–æ –≤ —ç—Ç–æ–º —á–∏—Å–ª–µ —Ü–∏—Ñ—Ä?   \n",
       "23118      –ß—Ç–æ –ø–æ–ª—É—á–∏—Ç—Å—è, –µ—Å–ª–∏ —ç—Ç–æ —á–∏—Å–ª–æ —É–º–Ω–æ–∂–∏—Ç—å –Ω–∞ 95?   \n",
       "14611               –ü—Ä–∏–±–∞–≤—å –∫ –Ω–µ–º—É 906, –∫–∞–∫–æ–π —Ä–µ–∑—É–ª—å—Ç–∞—Ç?   \n",
       "22726         –ß–µ–º—É —Ä–∞–≤–Ω–æ –ø—Ä–æ–∏–∑–≤–µ–¥–µ–Ω–∏–µ —ç—Ç–æ–≥–æ —á–∏—Å–ª–∞ –Ω–∞ 92?   \n",
       "44101  –ó–∞—á–µ—Ä–∫–Ω–∏ –≤ —ç—Ç–æ–º —Å–ø–∏—Å–∫–µ —É –∫–∞–∂–¥–æ–≥–æ —á–∏—Å–ª–∞ –ø–µ—Ä–≤—É—é ...   \n",
       "\n",
       "                                  context/1  \\\n",
       "18854                                    50   \n",
       "23118                                   114   \n",
       "14611                                     0   \n",
       "22726  –†–µ–∑—É–ª—å—Ç–∞—Ç —ç—Ç–æ–≥–æ –≤—ã—Ä–∞–∂–µ–Ω–∏—è —Ä–∞–≤–µ–Ω 1168   \n",
       "44101     9508, 5607, 3142, 2321, 2116, 458   \n",
       "\n",
       "                                               context/0  \n",
       "18854                 –ü—Ä–µ–∂–¥–µ –≤—Å–µ–≥–æ, 10*5 —á–µ–º—É —Ä–∞–≤–Ω—è–µ—Ç—Å—è?  \n",
       "23118  –ß—Ç–æ –Ω–∞–¥–æ –ø–æ–¥—Å—Ç–∞–≤–∏—Ç—å –≤–º–µ—Å—Ç–æ e, —á—Ç–æ–±—ã –≤—ã—Ä–∞–∂–µ–Ω–∏–µ ...  \n",
       "14611                –í–ª–∞–¥–∏—Å–ª–∞–≤, —Å–∫–∞–∂–∏, –∞ —á–µ–º—É —Ä–∞–≤–Ω–æ 4-4?  \n",
       "22726  –ü–æ—Å—á–∏—Ç–∞–π 3087024 / ( 25938402 / ( 5807 - 5637 ...  \n",
       "44101  –û—Ç—Å–æ—Ä—Ç–∏—Ä—É–π –ø–æ —É–±—ã–≤–∞–Ω–∏—é —á–∏—Å–ª–∞ –≤ —Å–ø–∏—Å–∫–µ 3142, 56...  "
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trn_df, val_df = train_test_split(df, test_size=0.1)\n",
    "trn_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# –ü–æ–¥–≥–æ—Ç–æ–≤–∫–∞ —Å–æ–ø—É—Ç—Å–≤—É—é—â–∏—Ö —à—Ç—É–∫: –¥–∞—Ç–∞—Å–µ—Ç—ã üê±‚Äçüêâ"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.nn.utils.rnn import pad_sequence\n",
    "from torch.utils.data import DataLoader, Dataset, RandomSampler, SequentialSampler\n",
    "from torch.utils.data.distributed import DistributedSampler\n",
    "from tqdm.notebook import tqdm, trange\n",
    "\n",
    "from pathlib import Path\n",
    "\n",
    "from transformers import (\n",
    "    MODEL_WITH_LM_HEAD_MAPPING,\n",
    "    WEIGHTS_NAME,\n",
    "    AdamW,\n",
    "    AutoConfig,\n",
    "    PreTrainedModel,\n",
    "    PreTrainedTokenizer,\n",
    "    get_linear_schedule_with_warmup,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def construct_conv(row, tokenizer, eos = True):\n",
    "    \"\"\"–°—Ç—Ä–æ–∏–º conversation\"\"\"\n",
    "    flatten = lambda l: [item for sublist in l for item in sublist]\n",
    "    conv = list(reversed([tokenizer.encode(x) + [tokenizer.eos_token_id] for x in row]))\n",
    "    conv = flatten(conv)\n",
    "    return conv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ConversationDataset(Dataset):\n",
    "    def __init__(self, tokenizer: PreTrainedTokenizer, args, df, block_size=512):\n",
    "\n",
    "        block_size = block_size - (tokenizer.model_max_length - tokenizer.max_len_single_sentence)\n",
    "\n",
    "        directory = args.cache_dir\n",
    "        cached_features_file = os.path.join(\n",
    "            directory, args.model_type + \"_cached_lm_\" + str(block_size)\n",
    "        )\n",
    "\n",
    "        if os.path.exists(cached_features_file) and not args.overwrite_cache:\n",
    "            logger.info(\"–ì—Ä—É—â–∏–º —Ñ–∏—á–∏ —Å –∫—ç—à–∞ %s\", cached_features_file)\n",
    "            with open(cached_features_file, \"rb\") as handle:\n",
    "                self.examples = pickle.load(handle)\n",
    "        else:\n",
    "            logger.info(\"–°–æ–∑–¥–∞–µ–º —Ñ–∏—á–∏ –≤ –¥–∏—Ä–µ–∫—Ç–æ—Ä–∏–∏ %s\", directory)\n",
    "\n",
    "            self.examples = []\n",
    "            for _, row in df.iterrows():\n",
    "                conv = construct_conv(row, tokenizer)\n",
    "                self.examples.append(conv)\n",
    "\n",
    "            logger.info(\"–°–æ—Ö—Ä–∞–Ω—è–µ–º —Ñ–∏—á–∏ –≤ —Ñ–∞–π–ª %s\", cached_features_file)\n",
    "            with open(cached_features_file, \"wb\") as handle:\n",
    "                pickle.dump(self.examples, handle, protocol=pickle.HIGHEST_PROTOCOL)\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.examples)\n",
    "\n",
    "    def __getitem__(self, item):\n",
    "        return torch.tensor(self.examples[item], dtype=torch.long)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_and_cache_examples(args, tokenizer, df_trn, df_val, evaluate=False):\n",
    "    return ConversationDataset(tokenizer, args, df_val if evaluate else df_trn)\n",
    "\n",
    "\n",
    "def set_seed(args):\n",
    "    random.seed(args.seed)\n",
    "    np.random.seed(args.seed)\n",
    "    torch.manual_seed(args.seed)\n",
    "    if args.n_gpu > 0:\n",
    "        torch.cuda.manual_seed_all(args.seed)\n",
    "\n",
    "\n",
    "def _sorted_checkpoints(args, checkpoint_prefix=\"checkpoint\", use_mtime=False):\n",
    "    ordering_and_checkpoint_path = []\n",
    "\n",
    "    glob_checkpoints = glob.glob(os.path.join(args.output_dir, \"{}-*\".format(checkpoint_prefix)))\n",
    "\n",
    "    for path in glob_checkpoints:\n",
    "        if use_mtime:\n",
    "            ordering_and_checkpoint_path.append((os.path.getmtime(path), path))\n",
    "        else:\n",
    "            regex_match = re.match(\".*{}-([0-9]+)\".format(checkpoint_prefix), path)\n",
    "            if regex_match and regex_match.groups():\n",
    "                ordering_and_checkpoint_path.append((int(regex_match.groups()[0]), path))\n",
    "\n",
    "    checkpoints_sorted = sorted(ordering_and_checkpoint_path)\n",
    "    checkpoints_sorted = [checkpoint[1] for checkpoint in checkpoints_sorted]\n",
    "    return checkpoints_sorted\n",
    "\n",
    "\n",
    "def _rotate_checkpoints(args, checkpoint_prefix=\"checkpoint\", use_mtime=False):\n",
    "    if not args.save_total_limit:\n",
    "        return\n",
    "    if args.save_total_limit <= 0:\n",
    "        return\n",
    "\n",
    "    checkpoints_sorted = _sorted_checkpoints(args, checkpoint_prefix, use_mtime)\n",
    "    if len(checkpoints_sorted) <= args.save_total_limit:\n",
    "        return\n",
    "\n",
    "    number_of_checkpoints_to_delete = max(0, len(checkpoints_sorted) - args.save_total_limit)\n",
    "    checkpoints_to_be_deleted = checkpoints_sorted[:number_of_checkpoints_to_delete]\n",
    "    for checkpoint in checkpoints_to_be_deleted:\n",
    "        logger.info(f\"–£–¥–∞–ª—è–µ–º —Å—Ç–∞—Ä—ã–π —á–µ–∫–ø–æ–∏–Ω—Ç [{checkpoint}], —Ä–æ—Ç–∏—Ä—É–µ–º, —Ç–∞–∫ —Å–∫–∞–∑–∞—Ç—å :)\")\n",
    "        shutil.rmtree(checkpoint)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# –ú–æ–¥–µ–ª—å! ü§ñ"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import glob\n",
    "import logging\n",
    "import os\n",
    "import pickle\n",
    "import random\n",
    "import re\n",
    "import shutil\n",
    "from torch.utils.tensorboard import SummaryWriter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\user\\anaconda3\\lib\\site-packages\\transformers\\models\\auto\\modeling_auto.py:1132: FutureWarning: The class `AutoModelWithLMHead` is deprecated and will be removed in a future version. Please use `AutoModelForCausalLM` for causal language models, `AutoModelForMaskedLM` for masked language models and `AutoModelForSeq2SeqLM` for encoder-decoder models.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoModelWithLMHead, AutoModelForCausalLM, AutoTokenizer\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"microsoft/DialoGPT-small\")\n",
    "model = AutoModelWithLMHead.from_pretrained(\"microsoft/DialoGPT-small\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "logger = logging.getLogger(__name__)\n",
    "\n",
    "MODEL_CONFIG_CLASSES = list(MODEL_WITH_LM_HEAD_MAPPING.keys())\n",
    "MODEL_TYPES = tuple(conf.model_type for conf in MODEL_CONFIG_CLASSES)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> –ö–æ–ª–¥—É–Ω—Å—Ç–≤–æ —Å –∞—Ä–≥—É–º–µ–Ω—Ç–∞–º–∏"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Args():\n",
    "    def __init__(self):\n",
    "        self.output_dir = '../our_gpt/content/output-small'\n",
    "        self.model_type = 'gpt2'\n",
    "        self.model_name_or_path = 'microsoft/DialoGPT-small'\n",
    "        self.config_name = 'microsoft/DialoGPT-small'\n",
    "        self.tokenizer_name = 'microsoft/DialoGPT-small'\n",
    "        self.cache_dir = '../our_gpt/cached'\n",
    "        self.block_size = 512\n",
    "        self.do_train = True\n",
    "        self.do_eval = True\n",
    "        self.evaluate_during_training = False\n",
    "        self.per_gpu_train_batch_size = 4\n",
    "        self.per_gpu_eval_batch_size = 4\n",
    "        self.gradient_accumulation_steps = 1\n",
    "        self.learning_rate = 5e-5\n",
    "        self.weight_decay = 0.0\n",
    "        self.adam_epsilon = 1e-8\n",
    "        self.max_grad_norm = 1.0\n",
    "        self.num_train_epochs = 20\n",
    "        self.max_steps = -1\n",
    "        self.warmup_steps = 0\n",
    "        self.logging_steps = 1000\n",
    "        self.save_steps = 3500\n",
    "        self.save_total_limit = None\n",
    "        self.eval_all_checkpoints = False\n",
    "        self.no_cuda = False\n",
    "        self.overwrite_output_dir = True\n",
    "        self.overwrite_cache = True\n",
    "        self.should_continue = False\n",
    "        self.seed = 42\n",
    "        self.local_rank = -1\n",
    "        self.fp16 = False\n",
    "        self.fp16_opt_level = 'O1'\n",
    "\n",
    "args = Args()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> –ü–æ—Ä—Ç—è–Ω–∫–∞ –Ω–∞–≥–ª–æ –≤–∑—è—Ç–∞ –≤ –æ—Ñ–∏—Ü–∏–∞–ª—å–Ω–æ–º —Ç—É—Ç–æ—Ä–∏–∞–ª–µ"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(args, train_dataset, model, tokenizer):\n",
    "    \"\"\" Train the model \"\"\"\n",
    "    if args.local_rank in [-1, 0]:\n",
    "        tb_writer = SummaryWriter()\n",
    "\n",
    "    args.train_batch_size = args.per_gpu_train_batch_size * max(1, args.n_gpu)\n",
    "\n",
    "    def collate(examples):\n",
    "        if tokenizer._pad_token is None:\n",
    "            return pad_sequence(examples, batch_first=True)\n",
    "        return pad_sequence(examples, batch_first=True, padding_value=tokenizer.pad_token_id)\n",
    "\n",
    "    train_sampler = RandomSampler(train_dataset) if args.local_rank == -1 else DistributedSampler(train_dataset)\n",
    "    train_dataloader = DataLoader(\n",
    "        train_dataset, sampler=train_sampler, batch_size=args.train_batch_size, collate_fn=collate, drop_last = True\n",
    "    )\n",
    "\n",
    "    if args.max_steps > 0:\n",
    "        t_total = args.max_steps\n",
    "        args.num_train_epochs = args.max_steps // (len(train_dataloader) // args.gradient_accumulation_steps) + 1\n",
    "    else:\n",
    "        t_total = len(train_dataloader) // args.gradient_accumulation_steps * args.num_train_epochs\n",
    "\n",
    "    model = model.module if hasattr(model, \"module\") else model  # Take care of distributed/parallel training\n",
    "    model.resize_token_embeddings(len(tokenizer))\n",
    "    # add_special_tokens_(model, tokenizer)\n",
    "\n",
    "\n",
    "    # Prepare optimizer and schedule (linear warmup and decay)\n",
    "    no_decay = [\"bias\", \"LayerNorm.weight\"]\n",
    "    optimizer_grouped_parameters = [\n",
    "        {\n",
    "            \"params\": [p for n, p in model.named_parameters() if not any(nd in n for nd in no_decay)],\n",
    "            \"weight_decay\": args.weight_decay,\n",
    "        },\n",
    "        {\"params\": [p for n, p in model.named_parameters() if any(nd in n for nd in no_decay)], \"weight_decay\": 0.0},\n",
    "    ]\n",
    "    optimizer = AdamW(optimizer_grouped_parameters, lr=args.learning_rate, eps=args.adam_epsilon)\n",
    "    scheduler = get_linear_schedule_with_warmup(\n",
    "        optimizer, num_warmup_steps=args.warmup_steps, num_training_steps=t_total\n",
    "    )\n",
    "\n",
    "    # Check if saved optimizer or scheduler states exist\n",
    "    if (\n",
    "        args.model_name_or_path\n",
    "        and os.path.isfile(os.path.join(args.model_name_or_path, \"optimizer.pt\"))\n",
    "        and os.path.isfile(os.path.join(args.model_name_or_path, \"scheduler.pt\"))\n",
    "    ):\n",
    "        # Load in optimizer and scheduler states\n",
    "        optimizer.load_state_dict(torch.load(os.path.join(args.model_name_or_path, \"optimizer.pt\")))\n",
    "        scheduler.load_state_dict(torch.load(os.path.join(args.model_name_or_path, \"scheduler.pt\")))\n",
    "\n",
    "    if args.fp16:\n",
    "        try:\n",
    "            from apex import amp\n",
    "        except ImportError:\n",
    "            raise ImportError(\"Please install apex from https://www.github.com/nvidia/apex to use fp16 training.\")\n",
    "        model, optimizer = amp.initialize(model, optimizer, opt_level=args.fp16_opt_level)\n",
    "\n",
    "    # multi-gpu training (should be after apex fp16 initialization)\n",
    "    if args.n_gpu > 1:\n",
    "        model = torch.nn.DataParallel(model)\n",
    "\n",
    "    # Distributed training (should be after apex fp16 initialization)\n",
    "    if args.local_rank != -1:\n",
    "        model = torch.nn.parallel.DistributedDataParallel(\n",
    "            model, device_ids=[args.local_rank], output_device=args.local_rank, find_unused_parameters=True\n",
    "        )\n",
    "\n",
    "    # Train!\n",
    "    logger.info(\"***** Running training *****\")\n",
    "    logger.info(\"  –ö–æ–ª–∏—á–µ—Å—Ç–≤–æ –ø—Ä–∏–º–µ—Ä–æ–≤ = %d\", len(train_dataset))\n",
    "    logger.info(\"  –≠–ø–æ—Ö–∞ = %d\", args.num_train_epochs)\n",
    "    logger.info(\"  –ë–∞—Ç—á –Ω–∞ GPU = %d\", args.per_gpu_train_batch_size)\n",
    "    logger.info(\n",
    "        \"  –û–±—â–∏–π –±–∞—Ç—á (—Å –∫–ª–∞—Å—Ç–µ—Ä–æ–º) = %d\",\n",
    "        args.train_batch_size\n",
    "        * args.gradient_accumulation_steps\n",
    "        * (torch.distributed.get_world_size() if args.local_rank != -1 else 1),\n",
    "    )\n",
    "    logger.info(\"  –®–∞–≥–∏ –∞–∫–∫–∞–º—É–ª—è—Ü–∏–∏ –≥—Ä–∞–¥–∏–µ–Ω—Ç–∞ = %d\", args.gradient_accumulation_steps)\n",
    "    logger.info(\"  –í—Å–µ–≥–æ —à–∞–≥–æ–≤ –æ–±—É—á–µ–Ω–∏—è = %d\", t_total)\n",
    "\n",
    "    global_step = 0\n",
    "    epochs_trained = 0\n",
    "    steps_trained_in_current_epoch = 0\n",
    "    # Check if continuing training from a checkpoint\n",
    "    if args.model_name_or_path and os.path.exists(args.model_name_or_path):\n",
    "        try:\n",
    "            # set global_step to gobal_step of last saved checkpoint from model path\n",
    "            checkpoint_suffix = args.model_name_or_path.split(\"-\")[-1].split(\"/\")[0]\n",
    "            global_step = int(checkpoint_suffix)\n",
    "            epochs_trained = global_step // (len(train_dataloader) // args.gradient_accumulation_steps)\n",
    "            steps_trained_in_current_epoch = global_step % (len(train_dataloader) // args.gradient_accumulation_steps)\n",
    "\n",
    "            logger.info(\"  –ü—Ä–æ–¥–æ–ª–∂–∞—é –æ–±—É—á–µ–Ω–∏–µ —Å —ç–ø–æ—Ö–∏ %d\", epochs_trained)\n",
    "            logger.info(\"  –ì–ª–æ–±–∞–ª—å–Ω—ã–π —à–∞–≥ %d\", global_step)\n",
    "            logger.info(\"  –ü—Ä–æ–ø—É—Å–∫–∞–µ–º %d —à–∞–≥–æ–≤ –Ω–∞ –ø–µ—Ä–≤–æ–π —ç–ø–æ—Ö–∏\", steps_trained_in_current_epoch)\n",
    "        except ValueError:\n",
    "            logger.info(\"  –ù–∞—á–∏–Ω–∞–µ–º —Ç—é–Ω–∏—Ç—å.\")\n",
    "\n",
    "    tr_loss, logging_loss = 0.0, 0.0\n",
    "\n",
    "    model.zero_grad()\n",
    "    train_iterator = trange(\n",
    "        epochs_trained, int(args.num_train_epochs), desc=\"–≠–ø–æ—Ö–∞\", disable=args.local_rank not in [-1, 0]\n",
    "    )\n",
    "    set_seed(args)  # Added here for reproducibility\n",
    "    for _ in train_iterator:\n",
    "        epoch_iterator = tqdm(train_dataloader, desc=\"–ò—Ç–µ—Ä–∞—Ü–∏—è\", disable=args.local_rank not in [-1, 0])\n",
    "        for step, batch in enumerate(epoch_iterator):\n",
    "\n",
    "            # Skip past any already trained steps if resuming training\n",
    "            if steps_trained_in_current_epoch > 0:\n",
    "                steps_trained_in_current_epoch -= 1\n",
    "                continue\n",
    "\n",
    "            inputs, labels = (batch, batch)\n",
    "            if inputs.shape[1] > 1024: continue\n",
    "            inputs = inputs.to(args.device)\n",
    "            labels = labels.to(args.device)\n",
    "            model.train()\n",
    "            outputs = model(inputs, labels=labels)\n",
    "            loss = outputs[0]  # model outputs are always tuple in transformers (see doc)\n",
    "\n",
    "            if args.n_gpu > 1:\n",
    "                loss = loss.mean()  # mean() to average on multi-gpu parallel training\n",
    "            if args.gradient_accumulation_steps > 1:\n",
    "                loss = loss / args.gradient_accumulation_steps\n",
    "\n",
    "            if args.fp16:\n",
    "                with amp.scale_loss(loss, optimizer) as scaled_loss:\n",
    "                    scaled_loss.backward()\n",
    "            else:\n",
    "                loss.backward()\n",
    "\n",
    "            tr_loss += loss.item()\n",
    "            if (step + 1) % args.gradient_accumulation_steps == 0:\n",
    "                if args.fp16:\n",
    "                    torch.nn.utils.clip_grad_norm_(amp.master_params(optimizer), args.max_grad_norm)\n",
    "                else:\n",
    "                    torch.nn.utils.clip_grad_norm_(model.parameters(), args.max_grad_norm)\n",
    "                optimizer.step()\n",
    "                scheduler.step()  # Update learning rate schedule\n",
    "                model.zero_grad()\n",
    "                global_step += 1\n",
    "\n",
    "                if args.local_rank in [-1, 0] and args.logging_steps > 0 and global_step % args.logging_steps == 0:\n",
    "                    # Log metrics\n",
    "                    if (\n",
    "                        args.local_rank == -1 and args.evaluate_during_training\n",
    "                    ):  # Only evaluate when single GPU otherwise metrics may not average well\n",
    "                        results = evaluate(args, model, tokenizer, df_trn, df_val)\n",
    "                        for key, value in results.items():\n",
    "                            tb_writer.add_scalar(\"eval_{}\".format(key), value, global_step)\n",
    "                    tb_writer.add_scalar(\"lr\", scheduler.get_lr()[0], global_step)\n",
    "                    tb_writer.add_scalar(\"loss\", (tr_loss - logging_loss) / args.logging_steps, global_step)\n",
    "                    logging_loss = tr_loss\n",
    "\n",
    "                if args.local_rank in [-1, 0] and args.save_steps > 0 and global_step % args.save_steps == 0:\n",
    "                    checkpoint_prefix = \"checkpoint\"\n",
    "                    # Save model checkpoint\n",
    "                    output_dir = os.path.join(args.output_dir, \"{}-{}\".format(checkpoint_prefix, global_step))\n",
    "                    os.makedirs(output_dir, exist_ok=True)\n",
    "                    model_to_save = (\n",
    "                        model.module if hasattr(model, \"module\") else model\n",
    "                    )  # Take care of distributed/parallel training\n",
    "                    model_to_save.save_pretrained(output_dir)\n",
    "                    tokenizer.save_pretrained(output_dir)\n",
    "\n",
    "                    torch.save(args, os.path.join(output_dir, \"training_args.bin\"))\n",
    "                    logger.info(\"–°–æ—Ö—Ä–∞–Ω—è—é —á–µ–∫–ø–æ–∏–Ω—Ç %s\", output_dir)\n",
    "\n",
    "                    _rotate_checkpoints(args, checkpoint_prefix)\n",
    "\n",
    "                    torch.save(optimizer.state_dict(), os.path.join(output_dir, \"optimizer.pt\"))\n",
    "                    torch.save(scheduler.state_dict(), os.path.join(output_dir, \"scheduler.pt\"))\n",
    "                    logger.info(\"–°–æ—Ö—Ä–∞–Ω—è—é –æ–ø—Ç–∏–º–∏–∑–µ—Ä –∏ —à–µ–ª–¥—É–ª–µ—Ä %s\", output_dir)\n",
    "\n",
    "            if args.max_steps > 0 and global_step > args.max_steps:\n",
    "                epoch_iterator.close()\n",
    "                break\n",
    "        if args.max_steps > 0 and global_step > args.max_steps:\n",
    "            train_iterator.close()\n",
    "            break\n",
    "\n",
    "    if args.local_rank in [-1, 0]:\n",
    "        tb_writer.close()\n",
    "\n",
    "    return global_step, tr_loss / global_step\n",
    "\n",
    "# Evaluation of some model\n",
    "\n",
    "def evaluate(args, model, tokenizer, df_trn, df_val, prefix=\"\"):\n",
    "    # Loop to handle MNLI double evaluation (matched, mis-matched)\n",
    "    eval_output_dir = args.output_dir\n",
    "\n",
    "    eval_dataset = load_and_cache_examples(args, tokenizer, df_trn, df_val, evaluate=True)\n",
    "    os.makedirs(eval_output_dir, exist_ok=True)\n",
    "    args.eval_batch_size = args.per_gpu_eval_batch_size * max(1, args.n_gpu)\n",
    "    # Note that DistributedSampler samples randomly\n",
    "\n",
    "    def collate(examples):\n",
    "        if tokenizer._pad_token is None:\n",
    "            return pad_sequence(examples, batch_first=True)\n",
    "        return pad_sequence(examples, batch_first=True, padding_value=tokenizer.pad_token_id)\n",
    "\n",
    "    eval_sampler = SequentialSampler(eval_dataset)\n",
    "    eval_dataloader = DataLoader(\n",
    "        eval_dataset, sampler=eval_sampler, batch_size=args.eval_batch_size, collate_fn=collate, drop_last = True\n",
    "    )\n",
    "\n",
    "    # multi-gpu evaluate\n",
    "    if args.n_gpu > 1:\n",
    "        model = torch.nn.DataParallel(model)\n",
    "\n",
    "    # Eval!\n",
    "    logger.info(\"***** –°—Ç–∞—Ä—Ç –≤–∞–ª–∏–¥–∞—Ü–∏–∏ {} *****\".format(prefix))\n",
    "    logger.info(\"  –ö–æ–ª–∏—á–µ—Å—Ç–≤–æ –æ–±—Ä–∞–∑—Ü–æ–≤ = %d\", len(eval_dataset))\n",
    "    logger.info(\"  –†–∞–∑–º–µ—Ä –±–∞—Ç—á–∞ = %d\", args.eval_batch_size)\n",
    "    eval_loss = 0.0\n",
    "    nb_eval_steps = 0\n",
    "    model.eval()\n",
    "\n",
    "    for batch in tqdm(eval_dataloader, desc=\"Evaluating\"):\n",
    "        inputs, labels = (batch, batch)\n",
    "        inputs = inputs.to(args.device)\n",
    "        labels = labels.to(args.device)\n",
    "\n",
    "        with torch.no_grad():\n",
    "            outputs = model(inputs, labels=labels)\n",
    "            lm_loss = outputs[0]\n",
    "            eval_loss += lm_loss.mean().item()\n",
    "        nb_eval_steps += 1\n",
    "\n",
    "    eval_loss = eval_loss / nb_eval_steps\n",
    "    perplexity = torch.exp(torch.tensor(eval_loss))\n",
    "\n",
    "    result = {\"perplexity\": perplexity}\n",
    "\n",
    "    output_eval_file = os.path.join(eval_output_dir, prefix, \"eval_results.txt\")\n",
    "    with open(output_eval_file, \"w\") as writer:\n",
    "        logger.info(\"***** –†–µ–∑—É–ª—å—Ç–∞—Ç—ã –≤–∞–ª–∏–¥–∞—Ü–∏–∏ {} *****\".format(prefix))\n",
    "        for key in sorted(result.keys()):\n",
    "            logger.info(\"  %s = %s\", key, str(result[key]))\n",
    "            writer.write(\"%s = %s\\n\" % (key, str(result[key])))\n",
    "\n",
    "    return result"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> –£—Ç–∞—â–µ–Ω–æ –æ—Ç—Ç—É–¥–∞ –∂–µ"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def main(df_trn, df_val):\n",
    "    args = Args()\n",
    "    \n",
    "    if args.should_continue:\n",
    "        sorted_checkpoints = _sorted_checkpoints(args)\n",
    "        if len(sorted_checkpoints) == 0:\n",
    "            raise ValueError(\"Used --should_continue but no checkpoint was found in --output_dir.\")\n",
    "        else:\n",
    "            args.model_name_or_path = sorted_checkpoints[-1]\n",
    "\n",
    "    if (\n",
    "        os.path.exists(args.output_dir)\n",
    "        and os.listdir(args.output_dir)\n",
    "        and args.do_train\n",
    "        and not args.overwrite_output_dir\n",
    "        and not args.should_continue\n",
    "    ):\n",
    "        raise ValueError(\n",
    "            \"Output directory ({}) already exists and is not empty. Use --overwrite_output_dir to overcome.\".format(\n",
    "                args.output_dir\n",
    "            )\n",
    "        )\n",
    "\n",
    "    # Setup CUDA, GPU & distributed training\n",
    "    device = torch.device(\"cuda\")\n",
    "    args.n_gpu = torch.cuda.device_count()\n",
    "    args.device = device\n",
    "\n",
    "    # Setup logging\n",
    "    logging.basicConfig(\n",
    "        format=\"%(asctime)s - %(levelname)s - %(name)s -   %(message)s\",\n",
    "        datefmt=\"%m/%d/%Y %H:%M:%S\",\n",
    "        level=logging.INFO if args.local_rank in [-1, 0] else logging.WARN,\n",
    "    )\n",
    "    logger.warning(\n",
    "        \"Process rank: %s, device: %s, n_gpu: %s, distributed training: %s, 16-bits training: %s\",\n",
    "        args.local_rank,\n",
    "        device,\n",
    "        args.n_gpu,\n",
    "        bool(args.local_rank != -1),\n",
    "        args.fp16,\n",
    "    )\n",
    "\n",
    "    # Set seed\n",
    "    set_seed(args)\n",
    "\n",
    "    config = AutoConfig.from_pretrained(args.config_name, cache_dir=args.cache_dir)\n",
    "    tokenizer = AutoTokenizer.from_pretrained(args.tokenizer_name, cache_dir=args.cache_dir)\n",
    "    model = AutoModelWithLMHead.from_pretrained(\n",
    "        args.model_name_or_path,\n",
    "        from_tf=False,\n",
    "        config=config,\n",
    "        cache_dir=args.cache_dir,\n",
    "    )\n",
    "    model.to(args.device)\n",
    "    \n",
    "    logger.info(\"Training/evaluation parameters %s\", args)\n",
    "\n",
    "    # Training\n",
    "    if args.do_train:\n",
    "        train_dataset = load_and_cache_examples(args, tokenizer, df_trn, df_val, evaluate=False)\n",
    "\n",
    "        global_step, tr_loss = train(args, train_dataset, model, tokenizer)\n",
    "        logger.info(\" global_step = %s, average loss = %s\", global_step, tr_loss)\n",
    "\n",
    "    # Saving best-practices: if you use save_pretrained for the model and tokenizer, you can reload them using from_pretrained()\n",
    "    if args.do_train:\n",
    "        # Create output directory if needed\n",
    "        os.makedirs(args.output_dir, exist_ok=True)\n",
    "\n",
    "        logger.info(\"Saving model checkpoint to %s\", args.output_dir)\n",
    "        # Save a trained model, configuration and tokenizer using `save_pretrained()`.\n",
    "        # They can then be reloaded using `from_pretrained()`\n",
    "        model_to_save = (\n",
    "            model.module if hasattr(model, \"module\") else model\n",
    "        )  # Take care of distributed/parallel training\n",
    "        model_to_save.save_pretrained(args.output_dir)\n",
    "        tokenizer.save_pretrained(args.output_dir)\n",
    "\n",
    "        # Good practice: save your training arguments together with the trained model\n",
    "        torch.save(args, os.path.join(args.output_dir, \"training_args.bin\"))\n",
    "\n",
    "        # Load a trained model and vocabulary that you have fine-tuned\n",
    "        model = AutoModelWithLMHead.from_pretrained(args.output_dir)\n",
    "        tokenizer = AutoTokenizer.from_pretrained(args.output_dir)\n",
    "        model.to(args.device)\n",
    "\n",
    "    # Evaluation\n",
    "    results = {}\n",
    "    if args.do_eval and args.local_rank in [-1, 0]:\n",
    "        checkpoints = [args.output_dir]\n",
    "        if args.eval_all_checkpoints:\n",
    "            checkpoints = list(\n",
    "                os.path.dirname(c) for c in sorted(glob.glob(args.output_dir + \"/**/\" + WEIGHTS_NAME, recursive=True))\n",
    "            )\n",
    "            logging.getLogger(\"transformers.modeling_utils\").setLevel(logging.WARN)  # Reduce logging\n",
    "        logger.info(\"Evaluate the following checkpoints: %s\", checkpoints)\n",
    "        for checkpoint in checkpoints:\n",
    "            global_step = checkpoint.split(\"-\")[-1] if len(checkpoints) > 1 else \"\"\n",
    "            prefix = checkpoint.split(\"/\")[-1] if checkpoint.find(\"checkpoint\") != -1 else \"\"\n",
    "\n",
    "            model = AutoModelWithLMHead.from_pretrained(checkpoint)\n",
    "            model.to(args.device)\n",
    "            result = evaluate(args, model, tokenizer, df_trn, df_val, prefix=prefix)\n",
    "            result = dict((k + \"_{}\".format(global_step), v) for k, v in result.items())\n",
    "            results.update(result)\n",
    "\n",
    "    return results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "05/21/2023 11:13:40 - WARNING - __main__ -   Process rank: -1, device: cuda, n_gpu: 1, distributed training: False, 16-bits training: False\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "99c854eb397a424bac886f61fea2126d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading:   0%|          | 0.00/641 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\user\\anaconda3\\lib\\site-packages\\huggingface_hub\\file_download.py:123: UserWarning: `huggingface_hub` cache-system uses symlinks by default to efficiently store duplicated files but your machine does not support them in E:\\learn\\urfum\\2 sem\\ProgIng2\\our_gpt\\cached. Caching files will still work but in a degraded version that might require more space on your disk. This warning can be disabled by setting the `HF_HUB_DISABLE_SYMLINKS_WARNING` environment variable. For more details, see https://huggingface.co/docs/huggingface_hub/how-to-cache#limitations.\n",
      "To support symlinks on Windows, you either need to activate Developer Mode or to run Python as an administrator. In order to see activate developer mode, see this article: https://docs.microsoft.com/en-us/windows/apps/get-started/enable-your-device-for-development\n",
      "  warnings.warn(message)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "fe7ceb6284de4d76ae06d930ef0ae5f1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading:   0%|          | 0.00/26.0 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a36f3121f18142a0a7aaef1d9717c4a0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading:   0%|          | 0.00/1.04M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "37cb4da32bcf4ee6bec3ef551589bf32",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading:   0%|          | 0.00/456k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3a3c2484143a486e827309e2f701363c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading:   0%|          | 0.00/351M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "05/21/2023 11:14:41 - INFO - __main__ -   Training/evaluation parameters <__main__.Args object at 0x000001F943C8E370>\n",
      "05/21/2023 11:14:41 - INFO - __main__ -   –°–æ–∑–¥–∞–µ–º —Ñ–∏—á–∏ –≤ –¥–∏—Ä–µ–∫—Ç–æ—Ä–∏–∏ ../our_gpt/cached\n",
      "05/21/2023 11:14:55 - INFO - __main__ -   –°–æ—Ö—Ä–∞–Ω—è–µ–º —Ñ–∏—á–∏ –≤ —Ñ–∞–π–ª ../our_gpt/cached\\gpt2_cached_lm_512\n",
      "C:\\Users\\user\\anaconda3\\lib\\site-packages\\transformers\\optimization.py:306: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n",
      "05/21/2023 11:14:57 - INFO - __main__ -   ***** Running training *****\n",
      "05/21/2023 11:14:57 - INFO - __main__ -     –ö–æ–ª–∏—á–µ—Å—Ç–≤–æ –ø—Ä–∏–º–µ—Ä–æ–≤ = 53826\n",
      "05/21/2023 11:14:57 - INFO - __main__ -     –≠–ø–æ—Ö–∞ = 20\n",
      "05/21/2023 11:14:57 - INFO - __main__ -     –ë–∞—Ç—á –Ω–∞ GPU = 4\n",
      "05/21/2023 11:14:57 - INFO - __main__ -     –û–±—â–∏–π –±–∞—Ç—á (—Å –∫–ª–∞—Å—Ç–µ—Ä–æ–º) = 4\n",
      "05/21/2023 11:14:57 - INFO - __main__ -     –®–∞–≥–∏ –∞–∫–∫–∞–º—É–ª—è—Ü–∏–∏ –≥—Ä–∞–¥–∏–µ–Ω—Ç–∞ = 1\n",
      "05/21/2023 11:14:57 - INFO - __main__ -     –í—Å–µ–≥–æ —à–∞–≥–æ–≤ –æ–±—É—á–µ–Ω–∏—è = 269120\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6f3f629e83b24f52bfad460413729255",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "–≠–ø–æ—Ö–∞:   0%|          | 0/20 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "af252a3bfcb94334a4b3701f71b68b01",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "–ò—Ç–µ—Ä–∞—Ü–∏—è:   0%|          | 0/13456 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "05/21/2023 11:25:22 - INFO - __main__ -   –°–æ—Ö—Ä–∞–Ω—è—é —á–µ–∫–ø–æ–∏–Ω—Ç ../our_gpt/content/output-small\\checkpoint-3500\n",
      "05/21/2023 11:25:28 - INFO - __main__ -   –°–æ—Ö—Ä–∞–Ω—è—é –æ–ø—Ç–∏–º–∏–∑–µ—Ä –∏ —à–µ–ª–¥—É–ª–µ—Ä ../our_gpt/content/output-small\\checkpoint-3500\n",
      "05/21/2023 11:34:49 - INFO - __main__ -   –°–æ—Ö—Ä–∞–Ω—è—é —á–µ–∫–ø–æ–∏–Ω—Ç ../our_gpt/content/output-small\\checkpoint-7000\n",
      "05/21/2023 11:34:54 - INFO - __main__ -   –°–æ—Ö—Ä–∞–Ω—è—é –æ–ø—Ç–∏–º–∏–∑–µ—Ä –∏ —à–µ–ª–¥—É–ª–µ—Ä ../our_gpt/content/output-small\\checkpoint-7000\n",
      "05/21/2023 11:44:21 - INFO - __main__ -   –°–æ—Ö—Ä–∞–Ω—è—é —á–µ–∫–ø–æ–∏–Ω—Ç ../our_gpt/content/output-small\\checkpoint-10500\n",
      "05/21/2023 11:44:26 - INFO - __main__ -   –°–æ—Ö—Ä–∞–Ω—è—é –æ–ø—Ç–∏–º–∏–∑–µ—Ä –∏ —à–µ–ª–¥—É–ª–µ—Ä ../our_gpt/content/output-small\\checkpoint-10500\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1c24d3c02dba4b0aa0b9d4b1d0a457e4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "–ò—Ç–µ—Ä–∞—Ü–∏—è:   0%|          | 0/13456 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "05/21/2023 11:53:58 - INFO - __main__ -   –°–æ—Ö—Ä–∞–Ω—è—é —á–µ–∫–ø–æ–∏–Ω—Ç ../our_gpt/content/output-small\\checkpoint-14000\n",
      "05/21/2023 11:54:01 - INFO - __main__ -   –°–æ—Ö—Ä–∞–Ω—è—é –æ–ø—Ç–∏–º–∏–∑–µ—Ä –∏ —à–µ–ª–¥—É–ª–µ—Ä ../our_gpt/content/output-small\\checkpoint-14000\n",
      "05/21/2023 12:03:26 - INFO - __main__ -   –°–æ—Ö—Ä–∞–Ω—è—é —á–µ–∫–ø–æ–∏–Ω—Ç ../our_gpt/content/output-small\\checkpoint-17500\n",
      "05/21/2023 12:03:32 - INFO - __main__ -   –°–æ—Ö—Ä–∞–Ω—è—é –æ–ø—Ç–∏–º–∏–∑–µ—Ä –∏ —à–µ–ª–¥—É–ª–µ—Ä ../our_gpt/content/output-small\\checkpoint-17500\n",
      "05/21/2023 12:12:58 - INFO - __main__ -   –°–æ—Ö—Ä–∞–Ω—è—é —á–µ–∫–ø–æ–∏–Ω—Ç ../our_gpt/content/output-small\\checkpoint-21000\n",
      "05/21/2023 12:13:03 - INFO - __main__ -   –°–æ—Ö—Ä–∞–Ω—è—é –æ–ø—Ç–∏–º–∏–∑–µ—Ä –∏ —à–µ–ª–¥—É–ª–µ—Ä ../our_gpt/content/output-small\\checkpoint-21000\n",
      "05/21/2023 12:22:27 - INFO - __main__ -   –°–æ—Ö—Ä–∞–Ω—è—é —á–µ–∫–ø–æ–∏–Ω—Ç ../our_gpt/content/output-small\\checkpoint-24500\n",
      "05/21/2023 12:22:31 - INFO - __main__ -   –°–æ—Ö—Ä–∞–Ω—è—é –æ–ø—Ç–∏–º–∏–∑–µ—Ä –∏ —à–µ–ª–¥—É–ª–µ—Ä ../our_gpt/content/output-small\\checkpoint-24500\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6dd39defcb7a4e6ea3065952b01991da",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "–ò—Ç–µ—Ä–∞—Ü–∏—è:   0%|          | 0/13456 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "05/21/2023 12:31:50 - INFO - __main__ -   –°–æ—Ö—Ä–∞–Ω—è—é —á–µ–∫–ø–æ–∏–Ω—Ç ../our_gpt/content/output-small\\checkpoint-28000\n",
      "05/21/2023 12:31:54 - INFO - __main__ -   –°–æ—Ö—Ä–∞–Ω—è—é –æ–ø—Ç–∏–º–∏–∑–µ—Ä –∏ —à–µ–ª–¥—É–ª–µ—Ä ../our_gpt/content/output-small\\checkpoint-28000\n",
      "05/21/2023 12:41:11 - INFO - __main__ -   –°–æ—Ö—Ä–∞–Ω—è—é —á–µ–∫–ø–æ–∏–Ω—Ç ../our_gpt/content/output-small\\checkpoint-31500\n",
      "05/21/2023 12:41:14 - INFO - __main__ -   –°–æ—Ö—Ä–∞–Ω—è—é –æ–ø—Ç–∏–º–∏–∑–µ—Ä –∏ —à–µ–ª–¥—É–ª–µ—Ä ../our_gpt/content/output-small\\checkpoint-31500\n",
      "05/21/2023 12:50:31 - INFO - __main__ -   –°–æ—Ö—Ä–∞–Ω—è—é —á–µ–∫–ø–æ–∏–Ω—Ç ../our_gpt/content/output-small\\checkpoint-35000\n",
      "05/21/2023 12:50:34 - INFO - __main__ -   –°–æ—Ö—Ä–∞–Ω—è—é –æ–ø—Ç–∏–º–∏–∑–µ—Ä –∏ —à–µ–ª–¥—É–ª–µ—Ä ../our_gpt/content/output-small\\checkpoint-35000\n",
      "05/21/2023 12:59:53 - INFO - __main__ -   –°–æ—Ö—Ä–∞–Ω—è—é —á–µ–∫–ø–æ–∏–Ω—Ç ../our_gpt/content/output-small\\checkpoint-38500\n",
      "05/21/2023 12:59:56 - INFO - __main__ -   –°–æ—Ö—Ä–∞–Ω—è—é –æ–ø—Ç–∏–º–∏–∑–µ—Ä –∏ —à–µ–ª–¥—É–ª–µ—Ä ../our_gpt/content/output-small\\checkpoint-38500\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2e12cdf611504814ac2a3ea7d0ba2c88",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "–ò—Ç–µ—Ä–∞—Ü–∏—è:   0%|          | 0/13456 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "05/21/2023 13:09:10 - INFO - __main__ -   –°–æ—Ö—Ä–∞–Ω—è—é —á–µ–∫–ø–æ–∏–Ω—Ç ../our_gpt/content/output-small\\checkpoint-42000\n",
      "05/21/2023 13:09:13 - INFO - __main__ -   –°–æ—Ö—Ä–∞–Ω—è—é –æ–ø—Ç–∏–º–∏–∑–µ—Ä –∏ —à–µ–ª–¥—É–ª–µ—Ä ../our_gpt/content/output-small\\checkpoint-42000\n",
      "05/21/2023 13:18:31 - INFO - __main__ -   –°–æ—Ö—Ä–∞–Ω—è—é —á–µ–∫–ø–æ–∏–Ω—Ç ../our_gpt/content/output-small\\checkpoint-45500\n",
      "05/21/2023 13:18:35 - INFO - __main__ -   –°–æ—Ö—Ä–∞–Ω—è—é –æ–ø—Ç–∏–º–∏–∑–µ—Ä –∏ —à–µ–ª–¥—É–ª–µ—Ä ../our_gpt/content/output-small\\checkpoint-45500\n",
      "05/21/2023 13:27:53 - INFO - __main__ -   –°–æ—Ö—Ä–∞–Ω—è—é —á–µ–∫–ø–æ–∏–Ω—Ç ../our_gpt/content/output-small\\checkpoint-49000\n",
      "05/21/2023 13:27:56 - INFO - __main__ -   –°–æ—Ö—Ä–∞–Ω—è—é –æ–ø—Ç–∏–º–∏–∑–µ—Ä –∏ —à–µ–ª–¥—É–ª–µ—Ä ../our_gpt/content/output-small\\checkpoint-49000\n",
      "05/21/2023 13:37:18 - INFO - __main__ -   –°–æ—Ö—Ä–∞–Ω—è—é —á–µ–∫–ø–æ–∏–Ω—Ç ../our_gpt/content/output-small\\checkpoint-52500\n",
      "05/21/2023 13:37:21 - INFO - __main__ -   –°–æ—Ö—Ä–∞–Ω—è—é –æ–ø—Ç–∏–º–∏–∑–µ—Ä –∏ —à–µ–ª–¥—É–ª–µ—Ä ../our_gpt/content/output-small\\checkpoint-52500\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2d44034a71b84a58a162e8612fbfb678",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "–ò—Ç–µ—Ä–∞—Ü–∏—è:   0%|          | 0/13456 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "05/21/2023 13:46:37 - INFO - __main__ -   –°–æ—Ö—Ä–∞–Ω—è—é —á–µ–∫–ø–æ–∏–Ω—Ç ../our_gpt/content/output-small\\checkpoint-56000\n",
      "05/21/2023 13:46:40 - INFO - __main__ -   –°–æ—Ö—Ä–∞–Ω—è—é –æ–ø—Ç–∏–º–∏–∑–µ—Ä –∏ —à–µ–ª–¥—É–ª–µ—Ä ../our_gpt/content/output-small\\checkpoint-56000\n",
      "05/21/2023 13:56:00 - INFO - __main__ -   –°–æ—Ö—Ä–∞–Ω—è—é —á–µ–∫–ø–æ–∏–Ω—Ç ../our_gpt/content/output-small\\checkpoint-59500\n",
      "05/21/2023 13:56:03 - INFO - __main__ -   –°–æ—Ö—Ä–∞–Ω—è—é –æ–ø—Ç–∏–º–∏–∑–µ—Ä –∏ —à–µ–ª–¥—É–ª–µ—Ä ../our_gpt/content/output-small\\checkpoint-59500\n",
      "05/21/2023 14:05:23 - INFO - __main__ -   –°–æ—Ö—Ä–∞–Ω—è—é —á–µ–∫–ø–æ–∏–Ω—Ç ../our_gpt/content/output-small\\checkpoint-63000\n",
      "05/21/2023 14:05:27 - INFO - __main__ -   –°–æ—Ö—Ä–∞–Ω—è—é –æ–ø—Ç–∏–º–∏–∑–µ—Ä –∏ —à–µ–ª–¥—É–ª–µ—Ä ../our_gpt/content/output-small\\checkpoint-63000\n",
      "05/21/2023 14:14:43 - INFO - __main__ -   –°–æ—Ö—Ä–∞–Ω—è—é —á–µ–∫–ø–æ–∏–Ω—Ç ../our_gpt/content/output-small\\checkpoint-66500\n",
      "05/21/2023 14:14:46 - INFO - __main__ -   –°–æ—Ö—Ä–∞–Ω—è—é –æ–ø—Ç–∏–º–∏–∑–µ—Ä –∏ —à–µ–ª–¥—É–ª–µ—Ä ../our_gpt/content/output-small\\checkpoint-66500\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b2bad4872d8746ab8b5f74f19b63becb",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "–ò—Ç–µ—Ä–∞—Ü–∏—è:   0%|          | 0/13456 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "05/21/2023 14:24:03 - INFO - __main__ -   –°–æ—Ö—Ä–∞–Ω—è—é —á–µ–∫–ø–æ–∏–Ω—Ç ../our_gpt/content/output-small\\checkpoint-70000\n",
      "05/21/2023 14:24:06 - INFO - __main__ -   –°–æ—Ö—Ä–∞–Ω—è—é –æ–ø—Ç–∏–º–∏–∑–µ—Ä –∏ —à–µ–ª–¥—É–ª–µ—Ä ../our_gpt/content/output-small\\checkpoint-70000\n",
      "05/21/2023 14:33:24 - INFO - __main__ -   –°–æ—Ö—Ä–∞–Ω—è—é —á–µ–∫–ø–æ–∏–Ω—Ç ../our_gpt/content/output-small\\checkpoint-73500\n",
      "05/21/2023 14:33:28 - INFO - __main__ -   –°–æ—Ö—Ä–∞–Ω—è—é –æ–ø—Ç–∏–º–∏–∑–µ—Ä –∏ —à–µ–ª–¥—É–ª–µ—Ä ../our_gpt/content/output-small\\checkpoint-73500\n",
      "05/21/2023 14:42:46 - INFO - __main__ -   –°–æ—Ö—Ä–∞–Ω—è—é —á–µ–∫–ø–æ–∏–Ω—Ç ../our_gpt/content/output-small\\checkpoint-77000\n",
      "05/21/2023 14:42:49 - INFO - __main__ -   –°–æ—Ö—Ä–∞–Ω—è—é –æ–ø—Ç–∏–º–∏–∑–µ—Ä –∏ —à–µ–ª–¥—É–ª–µ—Ä ../our_gpt/content/output-small\\checkpoint-77000\n",
      "05/21/2023 14:52:07 - INFO - __main__ -   –°–æ—Ö—Ä–∞–Ω—è—é —á–µ–∫–ø–æ–∏–Ω—Ç ../our_gpt/content/output-small\\checkpoint-80500\n",
      "05/21/2023 14:52:10 - INFO - __main__ -   –°–æ—Ö—Ä–∞–Ω—è—é –æ–ø—Ç–∏–º–∏–∑–µ—Ä –∏ —à–µ–ª–¥—É–ª–µ—Ä ../our_gpt/content/output-small\\checkpoint-80500\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "54cfefb99f61451bb4afbc1695e7615c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "–ò—Ç–µ—Ä–∞—Ü–∏—è:   0%|          | 0/13456 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "05/21/2023 15:01:28 - INFO - __main__ -   –°–æ—Ö—Ä–∞–Ω—è—é —á–µ–∫–ø–æ–∏–Ω—Ç ../our_gpt/content/output-small\\checkpoint-84000\n",
      "05/21/2023 15:01:31 - INFO - __main__ -   –°–æ—Ö—Ä–∞–Ω—è—é –æ–ø—Ç–∏–º–∏–∑–µ—Ä –∏ —à–µ–ª–¥—É–ª–µ—Ä ../our_gpt/content/output-small\\checkpoint-84000\n",
      "05/21/2023 15:10:50 - INFO - __main__ -   –°–æ—Ö—Ä–∞–Ω—è—é —á–µ–∫–ø–æ–∏–Ω—Ç ../our_gpt/content/output-small\\checkpoint-87500\n",
      "05/21/2023 15:10:52 - INFO - __main__ -   –°–æ—Ö—Ä–∞–Ω—è—é –æ–ø—Ç–∏–º–∏–∑–µ—Ä –∏ —à–µ–ª–¥—É–ª–µ—Ä ../our_gpt/content/output-small\\checkpoint-87500\n",
      "05/21/2023 15:20:12 - INFO - __main__ -   –°–æ—Ö—Ä–∞–Ω—è—é —á–µ–∫–ø–æ–∏–Ω—Ç ../our_gpt/content/output-small\\checkpoint-91000\n",
      "05/21/2023 15:20:15 - INFO - __main__ -   –°–æ—Ö—Ä–∞–Ω—è—é –æ–ø—Ç–∏–º–∏–∑–µ—Ä –∏ —à–µ–ª–¥—É–ª–µ—Ä ../our_gpt/content/output-small\\checkpoint-91000\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "13f0a0e14bd641dca06e1a1bf89d5d27",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "–ò—Ç–µ—Ä–∞—Ü–∏—è:   0%|          | 0/13456 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "05/21/2023 15:29:31 - INFO - __main__ -   –°–æ—Ö—Ä–∞–Ω—è—é —á–µ–∫–ø–æ–∏–Ω—Ç ../our_gpt/content/output-small\\checkpoint-94500\n",
      "05/21/2023 15:29:33 - INFO - __main__ -   –°–æ—Ö—Ä–∞–Ω—è—é –æ–ø—Ç–∏–º–∏–∑–µ—Ä –∏ —à–µ–ª–¥—É–ª–µ—Ä ../our_gpt/content/output-small\\checkpoint-94500\n",
      "05/21/2023 15:38:50 - INFO - __main__ -   –°–æ—Ö—Ä–∞–Ω—è—é —á–µ–∫–ø–æ–∏–Ω—Ç ../our_gpt/content/output-small\\checkpoint-98000\n",
      "05/21/2023 15:38:52 - INFO - __main__ -   –°–æ—Ö—Ä–∞–Ω—è—é –æ–ø—Ç–∏–º–∏–∑–µ—Ä –∏ —à–µ–ª–¥—É–ª–µ—Ä ../our_gpt/content/output-small\\checkpoint-98000\n",
      "05/21/2023 15:48:09 - INFO - __main__ -   –°–æ—Ö—Ä–∞–Ω—è—é —á–µ–∫–ø–æ–∏–Ω—Ç ../our_gpt/content/output-small\\checkpoint-101500\n",
      "05/21/2023 15:48:12 - INFO - __main__ -   –°–æ—Ö—Ä–∞–Ω—è—é –æ–ø—Ç–∏–º–∏–∑–µ—Ä –∏ —à–µ–ª–¥—É–ª–µ—Ä ../our_gpt/content/output-small\\checkpoint-101500\n",
      "05/21/2023 15:57:41 - INFO - __main__ -   –°–æ—Ö—Ä–∞–Ω—è—é —á–µ–∫–ø–æ–∏–Ω—Ç ../our_gpt/content/output-small\\checkpoint-105000\n",
      "05/21/2023 15:57:45 - INFO - __main__ -   –°–æ—Ö—Ä–∞–Ω—è—é –æ–ø—Ç–∏–º–∏–∑–µ—Ä –∏ —à–µ–ª–¥—É–ª–µ—Ä ../our_gpt/content/output-small\\checkpoint-105000\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "515955fc50d04cacbf2231c7ea0e252a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "–ò—Ç–µ—Ä–∞—Ü–∏—è:   0%|          | 0/13456 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "05/21/2023 16:07:11 - INFO - __main__ -   –°–æ—Ö—Ä–∞–Ω—è—é —á–µ–∫–ø–æ–∏–Ω—Ç ../our_gpt/content/output-small\\checkpoint-108500\n",
      "05/21/2023 16:07:15 - INFO - __main__ -   –°–æ—Ö—Ä–∞–Ω—è—é –æ–ø—Ç–∏–º–∏–∑–µ—Ä –∏ —à–µ–ª–¥—É–ª–µ—Ä ../our_gpt/content/output-small\\checkpoint-108500\n",
      "05/21/2023 16:16:43 - INFO - __main__ -   –°–æ—Ö—Ä–∞–Ω—è—é —á–µ–∫–ø–æ–∏–Ω—Ç ../our_gpt/content/output-small\\checkpoint-112000\n",
      "05/21/2023 16:16:48 - INFO - __main__ -   –°–æ—Ö—Ä–∞–Ω—è—é –æ–ø—Ç–∏–º–∏–∑–µ—Ä –∏ —à–µ–ª–¥—É–ª–µ—Ä ../our_gpt/content/output-small\\checkpoint-112000\n",
      "05/21/2023 16:26:14 - INFO - __main__ -   –°–æ—Ö—Ä–∞–Ω—è—é —á–µ–∫–ø–æ–∏–Ω—Ç ../our_gpt/content/output-small\\checkpoint-115500\n",
      "05/21/2023 16:26:17 - INFO - __main__ -   –°–æ—Ö—Ä–∞–Ω—è—é –æ–ø—Ç–∏–º–∏–∑–µ—Ä –∏ —à–µ–ª–¥—É–ª–µ—Ä ../our_gpt/content/output-small\\checkpoint-115500\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #800000; text-decoration-color: #800000\">‚ï≠‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ </span><span style=\"color: #800000; text-decoration-color: #800000; font-weight: bold\">Traceback </span><span style=\"color: #bf7f7f; text-decoration-color: #bf7f7f; font-weight: bold\">(most recent call last)</span><span style=\"color: #800000; text-decoration-color: #800000\"> ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚ïÆ</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">‚îÇ</span> <span style=\"color: #808000; text-decoration-color: #808000\">C:\\Users\\user\\AppData\\Local\\Temp\\ipykernel_8616\\1429600441.py</span>:<span style=\"color: #0000ff; text-decoration-color: #0000ff\">1</span> in <span style=\"color: #00ff00; text-decoration-color: #00ff00\">&lt;cell line: 1&gt;</span>                <span style=\"color: #800000; text-decoration-color: #800000\">‚îÇ</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">‚îÇ</span>                                                                                                  <span style=\"color: #800000; text-decoration-color: #800000\">‚îÇ</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">‚îÇ</span> <span style=\"color: #800000; text-decoration-color: #800000; font-style: italic\">[Errno 2] No such file or directory: </span>                                                            <span style=\"color: #800000; text-decoration-color: #800000\">‚îÇ</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">‚îÇ</span> <span style=\"color: #800000; text-decoration-color: #800000; font-style: italic\">'C:\\\\Users\\\\user\\\\AppData\\\\Local\\\\Temp\\\\ipykernel_8616\\\\1429600441.py'</span>                           <span style=\"color: #800000; text-decoration-color: #800000\">‚îÇ</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">‚îÇ</span>                                                                                                  <span style=\"color: #800000; text-decoration-color: #800000\">‚îÇ</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">‚îÇ</span> <span style=\"color: #808000; text-decoration-color: #808000\">C:\\Users\\user\\AppData\\Local\\Temp\\ipykernel_8616\\1910624677.py</span>:<span style=\"color: #0000ff; text-decoration-color: #0000ff\">63</span> in <span style=\"color: #00ff00; text-decoration-color: #00ff00\">main</span>                         <span style=\"color: #800000; text-decoration-color: #800000\">‚îÇ</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">‚îÇ</span>                                                                                                  <span style=\"color: #800000; text-decoration-color: #800000\">‚îÇ</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">‚îÇ</span> <span style=\"color: #800000; text-decoration-color: #800000; font-style: italic\">[Errno 2] No such file or directory: </span>                                                            <span style=\"color: #800000; text-decoration-color: #800000\">‚îÇ</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">‚îÇ</span> <span style=\"color: #800000; text-decoration-color: #800000; font-style: italic\">'C:\\\\Users\\\\user\\\\AppData\\\\Local\\\\Temp\\\\ipykernel_8616\\\\1910624677.py'</span>                           <span style=\"color: #800000; text-decoration-color: #800000\">‚îÇ</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">‚îÇ</span>                                                                                                  <span style=\"color: #800000; text-decoration-color: #800000\">‚îÇ</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">‚îÇ</span> <span style=\"color: #808000; text-decoration-color: #808000\">C:\\Users\\user\\AppData\\Local\\Temp\\ipykernel_8616\\4204762139.py</span>:<span style=\"color: #0000ff; text-decoration-color: #0000ff\">137</span> in <span style=\"color: #00ff00; text-decoration-color: #00ff00\">train</span>                       <span style=\"color: #800000; text-decoration-color: #800000\">‚îÇ</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">‚îÇ</span>                                                                                                  <span style=\"color: #800000; text-decoration-color: #800000\">‚îÇ</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">‚îÇ</span> <span style=\"color: #800000; text-decoration-color: #800000; font-style: italic\">[Errno 2] No such file or directory: </span>                                                            <span style=\"color: #800000; text-decoration-color: #800000\">‚îÇ</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">‚îÇ</span> <span style=\"color: #800000; text-decoration-color: #800000; font-style: italic\">'C:\\\\Users\\\\user\\\\AppData\\\\Local\\\\Temp\\\\ipykernel_8616\\\\4204762139.py'</span>                           <span style=\"color: #800000; text-decoration-color: #800000\">‚îÇ</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">‚ï∞‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚ïØ</span>\n",
       "<span style=\"color: #ff0000; text-decoration-color: #ff0000; font-weight: bold\">KeyboardInterrupt</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[31m‚ï≠‚îÄ\u001b[0m\u001b[31m‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ\u001b[0m\u001b[31m \u001b[0m\u001b[1;31mTraceback \u001b[0m\u001b[1;2;31m(most recent call last)\u001b[0m\u001b[31m \u001b[0m\u001b[31m‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ\u001b[0m\u001b[31m‚îÄ‚ïÆ\u001b[0m\n",
       "\u001b[31m‚îÇ\u001b[0m \u001b[33mC:\\Users\\user\\AppData\\Local\\Temp\\ipykernel_8616\\1429600441.py\u001b[0m:\u001b[94m1\u001b[0m in \u001b[92m<cell line: 1>\u001b[0m                \u001b[31m‚îÇ\u001b[0m\n",
       "\u001b[31m‚îÇ\u001b[0m                                                                                                  \u001b[31m‚îÇ\u001b[0m\n",
       "\u001b[31m‚îÇ\u001b[0m \u001b[3;31m[Errno 2] No such file or directory: \u001b[0m                                                            \u001b[31m‚îÇ\u001b[0m\n",
       "\u001b[31m‚îÇ\u001b[0m \u001b[3;31m'C:\\\\Users\\\\user\\\\AppData\\\\Local\\\\Temp\\\\ipykernel_8616\\\\1429600441.py'\u001b[0m                           \u001b[31m‚îÇ\u001b[0m\n",
       "\u001b[31m‚îÇ\u001b[0m                                                                                                  \u001b[31m‚îÇ\u001b[0m\n",
       "\u001b[31m‚îÇ\u001b[0m \u001b[33mC:\\Users\\user\\AppData\\Local\\Temp\\ipykernel_8616\\1910624677.py\u001b[0m:\u001b[94m63\u001b[0m in \u001b[92mmain\u001b[0m                         \u001b[31m‚îÇ\u001b[0m\n",
       "\u001b[31m‚îÇ\u001b[0m                                                                                                  \u001b[31m‚îÇ\u001b[0m\n",
       "\u001b[31m‚îÇ\u001b[0m \u001b[3;31m[Errno 2] No such file or directory: \u001b[0m                                                            \u001b[31m‚îÇ\u001b[0m\n",
       "\u001b[31m‚îÇ\u001b[0m \u001b[3;31m'C:\\\\Users\\\\user\\\\AppData\\\\Local\\\\Temp\\\\ipykernel_8616\\\\1910624677.py'\u001b[0m                           \u001b[31m‚îÇ\u001b[0m\n",
       "\u001b[31m‚îÇ\u001b[0m                                                                                                  \u001b[31m‚îÇ\u001b[0m\n",
       "\u001b[31m‚îÇ\u001b[0m \u001b[33mC:\\Users\\user\\AppData\\Local\\Temp\\ipykernel_8616\\4204762139.py\u001b[0m:\u001b[94m137\u001b[0m in \u001b[92mtrain\u001b[0m                       \u001b[31m‚îÇ\u001b[0m\n",
       "\u001b[31m‚îÇ\u001b[0m                                                                                                  \u001b[31m‚îÇ\u001b[0m\n",
       "\u001b[31m‚îÇ\u001b[0m \u001b[3;31m[Errno 2] No such file or directory: \u001b[0m                                                            \u001b[31m‚îÇ\u001b[0m\n",
       "\u001b[31m‚îÇ\u001b[0m \u001b[3;31m'C:\\\\Users\\\\user\\\\AppData\\\\Local\\\\Temp\\\\ipykernel_8616\\\\4204762139.py'\u001b[0m                           \u001b[31m‚îÇ\u001b[0m\n",
       "\u001b[31m‚ï∞‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚ïØ\u001b[0m\n",
       "\u001b[1;91mKeyboardInterrupt\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "main(trn_df, val_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# –í–∞–ª–∏–¥–∏—Ä—É–µ–º—Å—è üßæ"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained('microsoft/DialoGPT-small')\n",
    "model = AutoModelWithLMHead.from_pretrained('../our_gpt/content/output-small')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ">> User:x=2y\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ArifMan: 2\n",
      ">> User:–†–µ–∑—É–ª—å—Ç–∞—Ç –Ω—É–∂–Ω–æ —É–º–Ω–æ–∂–∏—Ç—å –Ω–∞ 16\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ArifMan: 32\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #800000; text-decoration-color: #800000\">‚ï≠‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ </span><span style=\"color: #800000; text-decoration-color: #800000; font-weight: bold\">Traceback </span><span style=\"color: #bf7f7f; text-decoration-color: #bf7f7f; font-weight: bold\">(most recent call last)</span><span style=\"color: #800000; text-decoration-color: #800000\"> ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚ïÆ</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">‚îÇ</span> <span style=\"color: #808000; text-decoration-color: #808000\">C:\\Users\\user\\AppData\\Local\\Temp\\ipykernel_10468\\265374008.py</span>:<span style=\"color: #0000ff; text-decoration-color: #0000ff\">2</span> in <span style=\"color: #00ff00; text-decoration-color: #00ff00\">&lt;cell line: 1&gt;</span>                <span style=\"color: #800000; text-decoration-color: #800000\">‚îÇ</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">‚îÇ</span>                                                                                                  <span style=\"color: #800000; text-decoration-color: #800000\">‚îÇ</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">‚îÇ</span> <span style=\"color: #800000; text-decoration-color: #800000; font-style: italic\">[Errno 2] No such file or directory: </span>                                                            <span style=\"color: #800000; text-decoration-color: #800000\">‚îÇ</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">‚îÇ</span> <span style=\"color: #800000; text-decoration-color: #800000; font-style: italic\">'C:\\\\Users\\\\user\\\\AppData\\\\Local\\\\Temp\\\\ipykernel_10468\\\\265374008.py'</span>                           <span style=\"color: #800000; text-decoration-color: #800000\">‚îÇ</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">‚îÇ</span>                                                                                                  <span style=\"color: #800000; text-decoration-color: #800000\">‚îÇ</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">‚îÇ</span> <span style=\"color: #808000; text-decoration-color: #808000\">C:\\Users\\user\\anaconda3\\lib\\site-packages\\ipykernel\\kernelbase.py</span>:<span style=\"color: #0000ff; text-decoration-color: #0000ff\">1075</span> in <span style=\"color: #00ff00; text-decoration-color: #00ff00\">raw_input</span>              <span style=\"color: #800000; text-decoration-color: #800000\">‚îÇ</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">‚îÇ</span>                                                                                                  <span style=\"color: #800000; text-decoration-color: #800000\">‚îÇ</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">‚îÇ</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">1072 ‚îÇ   ‚îÇ   ‚îÇ   </span><span style=\"color: #0000ff; text-decoration-color: #0000ff\">raise</span> StdinNotImplementedError(                                               <span style=\"color: #800000; text-decoration-color: #800000\">‚îÇ</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">‚îÇ</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">1073 ‚îÇ   ‚îÇ   ‚îÇ   ‚îÇ   </span><span style=\"color: #808000; text-decoration-color: #808000\">\"raw_input was called, but this frontend does not support input requests</span>  <span style=\"color: #800000; text-decoration-color: #800000\">‚îÇ</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">‚îÇ</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">1074 ‚îÇ   ‚îÇ   ‚îÇ   </span>)                                                                             <span style=\"color: #800000; text-decoration-color: #800000\">‚îÇ</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">‚îÇ</span> <span style=\"color: #800000; text-decoration-color: #800000\">‚ù± </span>1075 <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">‚îÇ   ‚îÇ   </span><span style=\"color: #0000ff; text-decoration-color: #0000ff\">return</span> <span style=\"color: #00ffff; text-decoration-color: #00ffff\">self</span>._input_request(                                                       <span style=\"color: #800000; text-decoration-color: #800000\">‚îÇ</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">‚îÇ</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">1076 ‚îÇ   ‚îÇ   ‚îÇ   </span><span style=\"color: #00ffff; text-decoration-color: #00ffff\">str</span>(prompt),                                                                  <span style=\"color: #800000; text-decoration-color: #800000\">‚îÇ</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">‚îÇ</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">1077 ‚îÇ   ‚îÇ   ‚îÇ   </span><span style=\"color: #00ffff; text-decoration-color: #00ffff\">self</span>._parent_ident[<span style=\"color: #808000; text-decoration-color: #808000\">\"shell\"</span>],                                                  <span style=\"color: #800000; text-decoration-color: #800000\">‚îÇ</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">‚îÇ</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">1078 ‚îÇ   ‚îÇ   ‚îÇ   </span><span style=\"color: #00ffff; text-decoration-color: #00ffff\">self</span>.get_parent(<span style=\"color: #808000; text-decoration-color: #808000\">\"shell\"</span>),                                                     <span style=\"color: #800000; text-decoration-color: #800000\">‚îÇ</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">‚îÇ</span>                                                                                                  <span style=\"color: #800000; text-decoration-color: #800000\">‚îÇ</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">‚îÇ</span> <span style=\"color: #808000; text-decoration-color: #808000\">C:\\Users\\user\\anaconda3\\lib\\site-packages\\ipykernel\\kernelbase.py</span>:<span style=\"color: #0000ff; text-decoration-color: #0000ff\">1120</span> in <span style=\"color: #00ff00; text-decoration-color: #00ff00\">_input_request</span>         <span style=\"color: #800000; text-decoration-color: #800000\">‚îÇ</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">‚îÇ</span>                                                                                                  <span style=\"color: #800000; text-decoration-color: #800000\">‚îÇ</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">‚îÇ</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">1117 ‚îÇ   ‚îÇ   ‚îÇ   ‚îÇ   ‚îÇ   ‚îÇ   </span><span style=\"color: #0000ff; text-decoration-color: #0000ff\">break</span>                                                             <span style=\"color: #800000; text-decoration-color: #800000\">‚îÇ</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">‚îÇ</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">1118 ‚îÇ   ‚îÇ   ‚îÇ   </span><span style=\"color: #0000ff; text-decoration-color: #0000ff\">except</span> <span style=\"color: #00ffff; text-decoration-color: #00ffff\">KeyboardInterrupt</span>:                                                     <span style=\"color: #800000; text-decoration-color: #800000\">‚îÇ</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">‚îÇ</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">1119 ‚îÇ   ‚îÇ   ‚îÇ   ‚îÇ   # re-raise KeyboardInterrupt, to truncate traceback</span>                       <span style=\"color: #800000; text-decoration-color: #800000\">‚îÇ</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">‚îÇ</span> <span style=\"color: #800000; text-decoration-color: #800000\">‚ù± </span>1120 <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">‚îÇ   ‚îÇ   ‚îÇ   ‚îÇ   </span><span style=\"color: #0000ff; text-decoration-color: #0000ff\">raise</span> <span style=\"color: #00ffff; text-decoration-color: #00ffff\">KeyboardInterrupt</span>(<span style=\"color: #808000; text-decoration-color: #808000\">\"Interrupted by user\"</span>) <span style=\"color: #0000ff; text-decoration-color: #0000ff\">from</span> <span style=\"color: #00ffff; text-decoration-color: #00ffff\">None</span>                  <span style=\"color: #800000; text-decoration-color: #800000\">‚îÇ</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">‚îÇ</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">1121 ‚îÇ   ‚îÇ   ‚îÇ   </span><span style=\"color: #0000ff; text-decoration-color: #0000ff\">except</span> <span style=\"color: #00ffff; text-decoration-color: #00ffff\">Exception</span>:                                                             <span style=\"color: #800000; text-decoration-color: #800000\">‚îÇ</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">‚îÇ</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">1122 ‚îÇ   ‚îÇ   ‚îÇ   ‚îÇ   </span><span style=\"color: #00ffff; text-decoration-color: #00ffff\">self</span>.log.warning(<span style=\"color: #808000; text-decoration-color: #808000\">\"Invalid Message:\"</span>, exc_info=<span style=\"color: #0000ff; text-decoration-color: #0000ff\">True</span>)                       <span style=\"color: #800000; text-decoration-color: #800000\">‚îÇ</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">‚îÇ</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">1123 </span>                                                                                          <span style=\"color: #800000; text-decoration-color: #800000\">‚îÇ</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">‚ï∞‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚ïØ</span>\n",
       "<span style=\"color: #ff0000; text-decoration-color: #ff0000; font-weight: bold\">KeyboardInterrupt: </span>Interrupted by user\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[31m‚ï≠‚îÄ\u001b[0m\u001b[31m‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ\u001b[0m\u001b[31m \u001b[0m\u001b[1;31mTraceback \u001b[0m\u001b[1;2;31m(most recent call last)\u001b[0m\u001b[31m \u001b[0m\u001b[31m‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ\u001b[0m\u001b[31m‚îÄ‚ïÆ\u001b[0m\n",
       "\u001b[31m‚îÇ\u001b[0m \u001b[33mC:\\Users\\user\\AppData\\Local\\Temp\\ipykernel_10468\\265374008.py\u001b[0m:\u001b[94m2\u001b[0m in \u001b[92m<cell line: 1>\u001b[0m                \u001b[31m‚îÇ\u001b[0m\n",
       "\u001b[31m‚îÇ\u001b[0m                                                                                                  \u001b[31m‚îÇ\u001b[0m\n",
       "\u001b[31m‚îÇ\u001b[0m \u001b[3;31m[Errno 2] No such file or directory: \u001b[0m                                                            \u001b[31m‚îÇ\u001b[0m\n",
       "\u001b[31m‚îÇ\u001b[0m \u001b[3;31m'C:\\\\Users\\\\user\\\\AppData\\\\Local\\\\Temp\\\\ipykernel_10468\\\\265374008.py'\u001b[0m                           \u001b[31m‚îÇ\u001b[0m\n",
       "\u001b[31m‚îÇ\u001b[0m                                                                                                  \u001b[31m‚îÇ\u001b[0m\n",
       "\u001b[31m‚îÇ\u001b[0m \u001b[33mC:\\Users\\user\\anaconda3\\lib\\site-packages\\ipykernel\\kernelbase.py\u001b[0m:\u001b[94m1075\u001b[0m in \u001b[92mraw_input\u001b[0m              \u001b[31m‚îÇ\u001b[0m\n",
       "\u001b[31m‚îÇ\u001b[0m                                                                                                  \u001b[31m‚îÇ\u001b[0m\n",
       "\u001b[31m‚îÇ\u001b[0m   \u001b[2m1072 \u001b[0m\u001b[2m‚îÇ   ‚îÇ   ‚îÇ   \u001b[0m\u001b[94mraise\u001b[0m StdinNotImplementedError(                                               \u001b[31m‚îÇ\u001b[0m\n",
       "\u001b[31m‚îÇ\u001b[0m   \u001b[2m1073 \u001b[0m\u001b[2m‚îÇ   ‚îÇ   ‚îÇ   ‚îÇ   \u001b[0m\u001b[33m\"\u001b[0m\u001b[33mraw_input was called, but this frontend does not support input requests\u001b[0m  \u001b[31m‚îÇ\u001b[0m\n",
       "\u001b[31m‚îÇ\u001b[0m   \u001b[2m1074 \u001b[0m\u001b[2m‚îÇ   ‚îÇ   ‚îÇ   \u001b[0m)                                                                             \u001b[31m‚îÇ\u001b[0m\n",
       "\u001b[31m‚îÇ\u001b[0m \u001b[31m‚ù± \u001b[0m1075 \u001b[2m‚îÇ   ‚îÇ   \u001b[0m\u001b[94mreturn\u001b[0m \u001b[96mself\u001b[0m._input_request(                                                       \u001b[31m‚îÇ\u001b[0m\n",
       "\u001b[31m‚îÇ\u001b[0m   \u001b[2m1076 \u001b[0m\u001b[2m‚îÇ   ‚îÇ   ‚îÇ   \u001b[0m\u001b[96mstr\u001b[0m(prompt),                                                                  \u001b[31m‚îÇ\u001b[0m\n",
       "\u001b[31m‚îÇ\u001b[0m   \u001b[2m1077 \u001b[0m\u001b[2m‚îÇ   ‚îÇ   ‚îÇ   \u001b[0m\u001b[96mself\u001b[0m._parent_ident[\u001b[33m\"\u001b[0m\u001b[33mshell\u001b[0m\u001b[33m\"\u001b[0m],                                                  \u001b[31m‚îÇ\u001b[0m\n",
       "\u001b[31m‚îÇ\u001b[0m   \u001b[2m1078 \u001b[0m\u001b[2m‚îÇ   ‚îÇ   ‚îÇ   \u001b[0m\u001b[96mself\u001b[0m.get_parent(\u001b[33m\"\u001b[0m\u001b[33mshell\u001b[0m\u001b[33m\"\u001b[0m),                                                     \u001b[31m‚îÇ\u001b[0m\n",
       "\u001b[31m‚îÇ\u001b[0m                                                                                                  \u001b[31m‚îÇ\u001b[0m\n",
       "\u001b[31m‚îÇ\u001b[0m \u001b[33mC:\\Users\\user\\anaconda3\\lib\\site-packages\\ipykernel\\kernelbase.py\u001b[0m:\u001b[94m1120\u001b[0m in \u001b[92m_input_request\u001b[0m         \u001b[31m‚îÇ\u001b[0m\n",
       "\u001b[31m‚îÇ\u001b[0m                                                                                                  \u001b[31m‚îÇ\u001b[0m\n",
       "\u001b[31m‚îÇ\u001b[0m   \u001b[2m1117 \u001b[0m\u001b[2m‚îÇ   ‚îÇ   ‚îÇ   ‚îÇ   ‚îÇ   ‚îÇ   \u001b[0m\u001b[94mbreak\u001b[0m                                                             \u001b[31m‚îÇ\u001b[0m\n",
       "\u001b[31m‚îÇ\u001b[0m   \u001b[2m1118 \u001b[0m\u001b[2m‚îÇ   ‚îÇ   ‚îÇ   \u001b[0m\u001b[94mexcept\u001b[0m \u001b[96mKeyboardInterrupt\u001b[0m:                                                     \u001b[31m‚îÇ\u001b[0m\n",
       "\u001b[31m‚îÇ\u001b[0m   \u001b[2m1119 \u001b[0m\u001b[2m‚îÇ   ‚îÇ   ‚îÇ   ‚îÇ   \u001b[0m\u001b[2m# re-raise KeyboardInterrupt, to truncate traceback\u001b[0m                       \u001b[31m‚îÇ\u001b[0m\n",
       "\u001b[31m‚îÇ\u001b[0m \u001b[31m‚ù± \u001b[0m1120 \u001b[2m‚îÇ   ‚îÇ   ‚îÇ   ‚îÇ   \u001b[0m\u001b[94mraise\u001b[0m \u001b[96mKeyboardInterrupt\u001b[0m(\u001b[33m\"\u001b[0m\u001b[33mInterrupted by user\u001b[0m\u001b[33m\"\u001b[0m) \u001b[94mfrom\u001b[0m \u001b[96mNone\u001b[0m                  \u001b[31m‚îÇ\u001b[0m\n",
       "\u001b[31m‚îÇ\u001b[0m   \u001b[2m1121 \u001b[0m\u001b[2m‚îÇ   ‚îÇ   ‚îÇ   \u001b[0m\u001b[94mexcept\u001b[0m \u001b[96mException\u001b[0m:                                                             \u001b[31m‚îÇ\u001b[0m\n",
       "\u001b[31m‚îÇ\u001b[0m   \u001b[2m1122 \u001b[0m\u001b[2m‚îÇ   ‚îÇ   ‚îÇ   ‚îÇ   \u001b[0m\u001b[96mself\u001b[0m.log.warning(\u001b[33m\"\u001b[0m\u001b[33mInvalid Message:\u001b[0m\u001b[33m\"\u001b[0m, exc_info=\u001b[94mTrue\u001b[0m)                       \u001b[31m‚îÇ\u001b[0m\n",
       "\u001b[31m‚îÇ\u001b[0m   \u001b[2m1123 \u001b[0m                                                                                          \u001b[31m‚îÇ\u001b[0m\n",
       "\u001b[31m‚ï∞‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚ïØ\u001b[0m\n",
       "\u001b[1;91mKeyboardInterrupt: \u001b[0mInterrupted by user\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "for step in range(4):\n",
    "    new_user_input_ids = tokenizer.encode(input(\">> User:\") + tokenizer.eos_token, return_tensors='pt')\n",
    "    bot_input_ids = torch.cat([chat_history_ids, new_user_input_ids], dim=-1) if step > 0 else new_user_input_ids\n",
    "\n",
    "    chat_history_ids = model.generate(\n",
    "        bot_input_ids, max_length=200,\n",
    "        pad_token_id=tokenizer.eos_token_id,  \n",
    "        no_repeat_ngram_size=3,       \n",
    "        do_sample=True, \n",
    "        top_k=100, \n",
    "        top_p=0.7,\n",
    "        temperature=0.8,\n",
    "    )\n",
    "    \n",
    "    # pretty print last ouput tokens from bot\n",
    "    print(\"ArifMan: {}\".format(tokenizer.decode(chat_history_ids[:, bot_input_ids.shape[-1]:][0], skip_special_tokens=True)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
